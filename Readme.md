<div align='center'>

![Magazord](image/logo-magazord.png)

</div>

# Teste para vaga de Engenheiro de Dados no Magazord.com.br
Este reposit√≥rio tem como fim testar os candidatos para vaga de engenheiro de dados na empresa [Magazord](https://magazord.com.br).
> Para esta vaga buscamos algu√©m apaixonado por Dados e como disponibilizar de maneira estrutura e eficiente para tomada de decis√£o sobre esses dados!


<details> <summary><strong>‚ò∞ SUM√ÅRIO</strong></summary>

## 1. Introdu√ß√£o  
- [Vis√£o Geral](#vis√£o-geral)  
- [Estrutura Final do Projeto](#estrutura-final-do-projeto)  
- [Tecnologias Utilizadas](#tecnologias-utilizadas)  
- [Depend√™ncias](#depend√™ncias)  

## 2. Configura√ß√£o e Execu√ß√£o  
- [Como Executar o Projeto](#como-executar-o-projeto)  
 
## 3. Pipeline ETL  
- [Como Funciona o ETL](#como-funciona-o-etl)  
  - [1Ô∏è‚É£ Extra√ß√£o (`extract.py`)](#1-extra√ß√£o-extractpy)  
  - [2Ô∏è‚É£ Transforma√ß√£o (`transform.py`)](#2-transforma√ß√£o-transformpy)  
  - [3Ô∏è‚É£ Carga (`load.py`)](#3-carga-loadpy)  

## 4. Configura√ß√£o do Docker  
- [Passo a Passo do Processo do Docker](#passo-a-passo-do-processo-do-docker)  
  - [1Ô∏è‚É£ Docker Compose (`docker-compose.yml`)](#1-docker-composeyml)  
  - [2Ô∏è‚É£ Dockerfile (`Dockerfile`)](#2-dockerfile)  

## 5. Passo a Passo dos Processos  
- [Processo de Extra√ß√£o](#passo-a-passo-do-processo-de-extra√ß√£o)  
- [Processo de Transforma√ß√£o](#passo-a-passo-do-processo-de-transforma√ß√£o)  
- [Processo de Carga](#passo-a-passo-do-processo-de-carga)  

## 6. Consultas Anal√≠ticas  
- [1Ô∏è‚É£ Receita Total por Categoria de Produto](#1-receita-total-por-categoria-de-produto)  
- [2Ô∏è‚É£ Top 5 Produtos Mais Vendidos em um Per√≠odo de Tempo](#top-5-produtos-mais-vendidos-em-um-per√≠odo-de-tempo)  
- [3Ô∏è‚É£ N√∫mero de Clientes Ativos nos √öltimos 3 Meses](#n√∫mero-de-clientes-ativos-que-fizeram-pelo-menos-1-compra-nos-√∫ltimos-3-meses)  

## 7. Otimiza√ß√£o do Pipeline  
- [1Ô∏è‚É£ Indexa√ß√£o nas Tabelas](#11-indexa√ß√£o-nas-tabelas-do-banco-de-dados)  
- [2Ô∏è‚É£ Particionamento de Dados](#12-estrat√©gias-de-particionamento)  
- [3Ô∏è‚É£ Paraleliza√ß√£o no Processo de Transforma√ß√£o](#13-paraleliza√ß√£o-no-processo-de-transforma√ß√£o)  

## 8. Decis√µes T√©cnicas  
- [1Ô∏è‚É£ Uso do PostgreSQL](#decis√µes-t√©cnicas)  
- [2Ô∏è‚É£ Armazenamento em Parquet](#decis√µes-t√©cnicas)  
- [3Ô∏è‚É£ Uso do Docker & Docker Compose](#decis√µes-t√©cnicas)  
- [4Ô∏è‚É£ Execu√ß√£o Sequencial dos Scripts ETL](#decis√µes-t√©cnicas)  

</details>

## Vis√£o Geral

Este projeto √© um pipeline ETL (Extract, Transform, Load) que processa dados de clientes, produtos e transa√ß√µes, aplica transforma√ß√µes e carrega os dados em um banco de dados PostgreSQL. O objetivo √© estruturar e disponibilizar os dados para tomada de decis√£o. O pipeline √© executado em um ambiente Docker, garantindo reprodutibilidade e facilidade de uso.

## Estrutura Final do Projeto

![Estrutura final](image/estrutura_projeto.png)


## Tecnologias Utilizadas

- Python 3.9: Linguagem principal para o pipeline ETL.
- Pandas: Manipula√ß√£o de dados.
- SQLAlchemy: Conex√£o com o banco de dados PostgreSQL.
- PostgreSQL 13: Banco de dados para armazenamento final.
- Docker: Containeriza√ß√£o do ambiente.
- Docker Compose: Orquestra√ß√£o dos servi√ßos (PostgreSQL e ETL).

## Depend√™ncias

As depend√™ncias do projeto est√£o listadas no arquivo requirements.txt:

- pandas
- requests
- sqlalchemy
- psycopg2-binary
- python-dotenv
- pyarrow

## Como Executar o Projeto

**Pr√©-requisitos**

- Docker: Instalado na m√°quina.
- Docker Compose: Geralmente vem com o Docker, mas certifique-se de que est√° instalado.

Passo a Passo:

**1. Clone ou fa√ßa o download do Reposit√≥rio**

- `git clone https://github.com/seu-usuario/seu-projeto.git`
- [datamart](https://github.com/seu-usuario/seu-projeto.git)
- `cd seu-projeto`

**2. Instale o Docker (Se Necess√°rio)**
Se voc√™ ainda n√£o tem o Docker instalado, siga as instru√ß√µes abaixo:

* Linux:
```
sudo apt update
sudo apt install docker.io docker-compose
sudo systemctl start docker
sudo systemctl enable docker
```

* Windows/Mac:

Baixe e instale o Docker Desktop a partir do site oficial: <a href="https://www.docker.com/get-started/" target="_blank">Docker</a>


**3. Definir Usu√°rio, Senha, Porta e Nome do Banco de dados**

* Abrir o arquivo `.env`
  - `DB_URL=postgresql://seunome:suasenha@db:5432/nomedobancodedados`
  - Estou passando como padr√£o, minhas informa√ß√µes:`DB_URL=postgresql://lucas:1234@db:5432/datamart`
  - Usu√°rio: Lucas
  - Senha: 1234
  - Porta: 5432
  - Nome do Banco de dados: datamart


**4. Construa e Inicie os Cont√™ineres**

- No diret√≥rio raiz do projeto, execute:

`docker-compose up --build`


**Isso ir√°:**

- Construir a imagem do cont√™iner do pipeline ETL.
- Iniciar o cont√™iner do PostgreSQL.
- Executar o pipeline ETL (extra√ß√£o, transforma√ß√£o e carregamento).


**5. Verifique os Dados no PostgreSQL**

Ap√≥s a execu√ß√£o do pipeline, os dados estar√£o dispon√≠veis no banco de dados PostgreSQL. Voc√™ pode acessar o banco de dados usando o terminal ou uma ferramenta como pgAdmin ou psql.

Host: localhost

Porta: 5432

Banco de dados: meubanco

Usu√°rio: usuario

Senha: senha

**Acesso Manual ao Banco de Dados no Terminal**

`docker start postgres_db`
`docker exec -it postgres_db psql -U usuario -d meubanco`

**6. Parar os Cont√™ineres**

Para parar os cont√™ineres, execute:

`docker-compose down`


## Como Funciona o ETL

### **1. Extra√ß√£o (extract.py)**

O script extract.py tem como objetivo baixar um arquivo ZIP do GitHub, extrair seus conte√∫dos e organizar os arquivos de dados em uma estrutura espec√≠fica dentro do diret√≥rio data/raw/.

- Baixa um arquivo ZIP do reposit√≥rio no GitHub.
- Extrai os arquivos contidos no ZIP.
- Move arquivos CSV soltos para data/raw.
- Encontra e extrai arquivos ZIP internos.
- Remove arquivos tempor√°rios.

### **2. Transforma√ß√£o (transform.py)**

Responsabilidade: Aplica transforma√ß√µes nos dados brutos.

- Carrega os arquivos CSV extra√≠dos para DataFrames do Pandas.
- Verifica valores nulos e os preenche adequadamente.
- Remove duplicatas.
- Ajusta e padroniza os dados:
  - Concatena nome e sobrenome dos clientes.
  - Renomeia colunas dos produtos.
  - Concatena transa√ß√µes de m√∫ltiplos arquivos.
- Salva os dados transformados no formato Parquet na pasta data/trusted.

### **3. Carga (load.py)**

Responsabilidade: Carrega os dados transformados no banco de dados PostgreSQL.

- Cria o schema meu_schema no PostgreSQL.
- Cria as tabelas clientes, produtos e transacoes.
- Carrega os dados transformados nas tabelas do PostgreSQL.


## Passo a Passo do Processo do Docker

Este projeto utiliza Docker para containeriza√ß√£o, garantindo um ambiente isolado e reprodut√≠vel e Docker Compose para orquestra√ß√£o dos servi√ßos.

### 1. Docker-compose.yml

- O docker-compose.yml gerencia a orquestra√ß√£o dos servi√ßos, incluindo um banco de dados PostgreSQL e o pipeline ETL.
- O docker-compose.yml define dois servi√ßos principais:
  - db: Um cont√™iner PostgreSQL 13 para armazenar os dados.
  - etl-pipeline: Um cont√™iner que executa o pipeline ETL.

#### 1.1 Defini√ß√£o dos Servi√ßos

```dockerfile
services:
  db:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_USER: usuario
      POSTGRES_PASSWORD: senha
      POSTGRES_DB: meubanco
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  etl-pipeline:
    build: .
    volumes:
      - ./data:/app/data
    environment:
      DB_URL: postgresql://usuario:senha@db:5432/meubanco
    command: bash -c "python src/extract.py && python src/transform.py && python src/load.py"
    depends_on:
      - db

volumes:
  postgres_data:
```

#### 1.2 Explica√ß√£o dos Par√¢metros

- db: Define um cont√™iner com PostgreSQL 13.
- O banco de dados usa credenciais definidas via environment.
- Exp√µe a porta 5432.
- Os dados s√£o persistidos no volume postgres_data.
- etl-pipeline: Cont√™iner respons√°vel por executar o pipeline ETL.
- Mapeia a pasta data/ para /app/data dentro do cont√™iner.
- Define a URL de conex√£o ao banco de dados via DB_URL.
- O command executa os scripts ETL (extract.py, transform.py e load.py).
- depends_on garante que o banco de dados seja iniciado antes do ETL.

### 2. Dockerfile

O Dockerfile √© respons√°vel por construir a imagem do pipeline ETL.

#### 2.1 Defini√ß√£o do Dockerfile

```dockerfile
FROM python:3.9
WORKDIR /app
COPY . .
RUN pip install --no-cache-dir -r requirements.txt
CMD ["bash", "-c", "python src/extract.py && python src/transform.py && python src/load.py"]
```
#### 2.2 Explica√ß√£o dos Par√¢metros

- `FROM python:3.9`: Usa uma imagem base com Python 3.9.
- `WORKDIR /app`: Define /app como diret√≥rio de trabalho.
- `COPY . .`: Copia todos os arquivos do projeto para o cont√™iner.
- `RUN pip install --no-cache-dir -r requirements.txt`: Instala as depend√™ncias.
- `CMD [...]`: Define a execu√ß√£o padr√£o dos scripts ETL.

### Conclus√£o

O docker-compose.yml e o Dockerfile garantem um ambiente configurado para execu√ß√£o automatizada do pipeline ETL. O PostgreSQL √© iniciado e mant√©m os dados persistentes, enquanto o cont√™iner ETL processa os dados de forma sequencial.

## Passo a Passo do Processo de Extra√ß√£o

O script extract.py tem como objetivo baixar um arquivo ZIP do GitHub, extrair seus conte√∫dos e organizar os arquivos de dados em uma estrutura espec√≠fica dentro do diret√≥rio `data/raw/`.

### Bibliotecas Utilizadas

- O script utiliza as seguintes bibliotecas:
- `requests`: Para fazer o download do arquivo ZIP.
- `zipfile`: Para manipular arquivos ZIP.
- `os`: Para manipula√ß√£o de diret√≥rios e arquivos.
- `shutil`: Para mover e deletar arquivos e pastas.

### 1. Baixar o ZIP do GitHub
O script baixa um arquivo ZIP diretamente do reposit√≥rio do GitHub e o salva localmente como `master.zip`.
```python
url = "https://github.com/magazord-plataforma/data_engineer_test/archive/refs/heads/master.zip"
zip_path = "master.zip"

with open(zip_path, "wb") as f:
    f.write(requests.get(url).content)
```

### 2. Extrair o ZIP principal
O ZIP baixado √© extra√≠do na pasta atual.
```python
with zipfile.ZipFile(zip_path, "r") as z:
    z.extractall(".")
```

### 3. Definir Caminhos
S√£o definidos os caminhos das pastas relevantes para a organiza√ß√£o dos arquivos extra√≠dos:
```python
folder = "data_engineer_test-master"  # Pasta extra√≠da
data_raw = "data/raw"  # Caminho para a pasta raw
os.makedirs(data_raw, exist_ok=True)  # Cria a pasta se n√£o existir
```

### 4. Mover os arquivos CSV soltos
Os arquivos `cliente.csv` e `produtos.csv` s√£o movidos para `data/raw/`, caso existam.
```python
for file in ["cliente.csv", "produtos.csv"]:
    file_path = os.path.join(folder, file)
    if os.path.exists(file_path):
        shutil.move(file_path, os.path.join(data_raw, file))
        print(f"‚úÖ {file} movido para {data_raw}")
```

### 5. Encontrar e extrair os ZIPs internos
Os arquivos ZIP internos (`transacoes_1.zip`, `transacoes_2.zip`, etc.) s√£o extra√≠dos diretamente para `data/raw/`.
```python
for file in os.listdir(folder):
    if file.endswith(".zip"):
        with zipfile.ZipFile(os.path.join(folder, file), "r") as z:
            z.extractall(data_raw)
```

### 6. Limpeza
Ap√≥s a extra√ß√£o e organiza√ß√£o dos arquivos, o script remove o ZIP original e a pasta extra√≠da para manter o diret√≥rio limpo.
```python
os.remove(zip_path)
shutil.rmtree(folder)
```

### 7. Mensagem Final
Ao concluir todas as opera√ß√µes, o script imprime uma mensagem de sucesso:
```python
print("üéâ Todos os arquivos foram movidos para 'data/raw/'")
```

### Conclus√£o
O script automatiza o processo de download, extra√ß√£o e organiza√ß√£o dos arquivos de dados, garantindo que todos os arquivos CSV estejam centralizados na pasta `data/raw/`. Esse procedimento facilita a pr√≥xima etapa do pipeline ETL.


## Passo a Passo do Processo de Transforma√ß√£o

O script transform.py tem como objetivo realizar a transforma√ß√£o dos dados brutos armazenados na pasta data/raw e gerar arquivos no formato Parquet na pasta `data/trusted`.


### Bibliotecas Utilizadas

O script utiliza as seguintes bibliotecas:

- `os`: Para manipula√ß√£o de diret√≥rios.
- `pandas`: Para manipula√ß√£o e transforma√ß√£o dos dados.
- `logging`: Para registrar mensagens sobre o andamento do processo.

### 1. Configura√ß√£o Inicial

Define os diret√≥rios e cria a pasta data/trusted caso ela n√£o exista:

```python
raw_path  = "data/raw"
trusted_path = "data/trusted"
os.makedirs(trusted_path, exist_ok=True)
```

### 2. Carregar os Arquivos CSV

Os arquivos CSV localizados em data/raw s√£o carregados como DataFrames do pandas:

```python
arquivos = [f for f in os.listdir(raw_path) if f.endswith('.csv')]
dfs = {arquivo: pd.read_csv(os.path.join(raw_path, arquivo)) for arquivo in arquivos}
```

### 3. Verifica√ß√£o de Valores Nulos

O script verifica se existem valores nulos e registra um aviso no logging caso existam:

```python
def verificar_nulos(dfs):
    for arquivo, df in dfs.items():
        nulos_existentes = df.isnull().sum()[df.isnull().sum() > 0]
        if not nulos_existentes.empty:
            logging.warning(f"‚ö†Ô∏è {arquivo} cont√©m valores nulos:\n{nulos_existentes}")
        else:
            logging.info(f"‚úÖ {arquivo}: sem valores nulos.")
verificar_nulos(dfs)
```

### 4. Tratamento de Valores Nulos

Os valores nulos s√£o preenchidos com:

- Desconhecido para colunas de texto.
- 0 para colunas num√©ricas.
- 1970-01-01 para colunas de data.

### 5. Remo√ß√£o de Duplicatas

O script remove registros duplicados de cada DataFrame:

```python
for arquivo in dfs:
    dfs[arquivo] = dfs[arquivo].drop_duplicates()
```

### 6. Transformando os Dados

* Cliente
  - Une os campos nome e sobrenome em um √∫nico campo nome_cliente.
  - Remove as colunas nome e sobrenome.
  - Renomeia colunas para um padr√£o mais coerente.

```python
dfs['cliente.csv'] = (
    dfs['cliente.csv']
    .assign(nome_cliente=lambda df: (df['nome'] + ' ' + df['sobrenome']).str.title())
    .drop(columns=['nome', 'sobrenome'])
    .rename(columns={'id': 'id_cliente', 'telefone': 'telefone', 'email': 'email'})
)
```

* Produtos
  - Remove a coluna EAN.
  - Renomeia colunas para padroniza√ß√£o.

```python
dfs['produtos.csv'] = (
    dfs['produtos.csv']
    .drop(columns=['EAN'])
    .rename(columns={'id': 'id_produto', 'Nome': 'nome_produto', 'Descri√ß√£o': 'categoria', 'Pre√ßo': 'preco'})
)
```

* Transa√ß√µes
  - Concatena os arquivos de transa√ß√µes (transacoes_1.csv, transacoes_2.csv, transacoes_3.csv).
  - Converte a coluna data_transacao para o formato de data.
  - Remove duplicatas.

```python
transacoes = (
    pd.concat([dfs['transacoes_1.csv'], dfs['transacoes_2.csv'], dfs['transacoes_3.csv']], ignore_index=True)
    .assign(data_transacao=lambda df: pd.to_datetime(df['data_transacao']).dt.date)
    .drop_duplicates()
)
```

### 7. Salvar os Dados Transformados em Parquet

Os arquivos transformados s√£o salvos na pasta `data/trusted` em formato Parquet.

```python
arquivos_para_salvar = {
    'cliente.parquet': dfs['cliente.csv'],
    'produtos.parquet': dfs['produtos.csv'],
    'transacoes.parquet': transacoes
}
for nome_arquivo, df in arquivos_para_salvar.items():
    parquet_file = os.path.join(trusted_path, nome_arquivo)
    df.to_parquet(parquet_file, index=False, compression='snappy', engine='pyarrow')
    print(f"Arquivo salvo: {parquet_file}")
```

### 8. Visualiza√ß√£o dos Dados

Por fim, o script imprime as 5 primeiras linhas de cada arquivo transformado.

```python
for nome_arquivo, df in arquivos_para_salvar.items():
    print(f"\nüîπ {nome_arquivo} (Top 5 linhas, {df.shape[0]} linhas no total):")
    print(df.head())
```
### Conclus√£o

O script transform.py realiza um pipeline de limpeza e transforma√ß√£o dos dados, garantindo a integridade e qualidade antes de serem utilizados para an√°lise ou carga em um banco de dados. Ele estrutura os dados de forma padronizada e os armazena em formato Parquet, que √© mais eficiente para processamento de grandes volumes de informa√ß√£o.


## Passo a Passo do Processo de Carga

O script load.py tem como objetivo carregar os dados transformados armazenados na pasta data/trusted para um banco de dados PostgreSQL, garantindo a estrutura correta das tabelas e a integridade dos dados.

### Bibliotecas Utilizadas

O script utiliza as seguintes bibliotecas:

- `os`: Para manipula√ß√£o de diret√≥rios.
- `dotenv`: Para carregar vari√°veis de ambiente do arquivo .env.
- `pandas`: Para manipula√ß√£o dos dados.
- `sqlalchemy`: Para conex√£o e inser√ß√£o de dados no banco PostgreSQL.

### 1. Configura√ß√µes Iniciais

#### 1.1 Carregar as Vari√°veis de Ambiente

O script carrega a URL do banco de dados a partir do arquivo `.env`:

```python
from dotenv import load_dotenv
import os

load_dotenv()
db_url = os.getenv('DB_URL')

if db_url is None:
    raise ValueError("A vari√°vel de ambiente 'DB_URL' n√£o est√° definida no arquivo .env.")
```

#### 1.2 Criar a Conex√£o com o Banco de Dados

A conex√£o com PostgreSQL √© estabelecida usando SQLAlchemy:

```python
from sqlalchemy import create_engine, text
engine = create_engine(db_url)
```

### 2. Verifica√ß√£o e Cria√ß√£o de Estrutura

O script verifica se o schema meu_schema j√° existe, criando-o se necess√°rio.

```python
with engine.begin() as conn:
    conn.execute(text("CREATE SCHEMA IF NOT EXISTS meu_schema"))
    print("‚úÖ Schema 'meu_schema' criado/verificado com sucesso!")
```

### 3. Carregamento dos Arquivos Parquet

#### 3.1 Definir Diret√≥rios

- Os diret√≥rios de entrada e sa√≠da s√£o configurados:

```python
trusted_path = '/app/data/trusted'
refined_path = '/app/data/refined'
os.makedirs(refined_path, exist_ok=True)
```

- Se a pasta trusted n√£o existir, o script lan√ßa um erro:

```python
if not os.path.exists(trusted_path):
    raise FileNotFoundError(f"‚ùå O diret√≥rio '{trusted_path}' n√£o foi encontrado!")

```

#### 3.2 Listar Arquivos Dispon√≠veis

O script lista os arquivos Parquet na pasta trusted:

```python
parquet_files = [f for f in os.listdir(trusted_path) if f.endswith('.parquet')]

if not parquet_files:
    print("‚ö†Ô∏è Nenhum arquivo Parquet encontrado na pasta 'trusted'. Nada para processar.")
else:
    print(f"üìÇ {len(parquet_files)} arquivos Parquet encontrados. Iniciando o processamento...")
```

#### 3.3 Definir Esquema das Tabelas

O esquema das tabelas √© definido para garantir que os tipos de dados estejam corretos ao carregar no PostgreSQL.

```python
from sqlalchemy import Integer, String, Float, Date, DECIMAL

schemas = {
    "clientes": {
        "id_cliente": Integer,
        "nome_cliente": String(100),
        "email": String(100),
        "telefone": String(15)
    },
    "produtos": {
        "id_produto": Integer,
        "nome_produto": String(100),
        "categoria": String(120),
        "preco": DECIMAL(10, 2)
    },
    "transacoes": {
        "id_transacao": Integer,
        "id_cliente": Integer,
        "id_produto": Integer,
        "quantidade": Integer,
        "data_transacao": Date
    }
}
```

### 4. Processar e Inserir os Dados no Banco

O script percorre os arquivos Parquet e os insere no banco de dados, garantindo que os tipos de dados estejam corretos antes da inser√ß√£o.:

```python
for parquet_file in parquet_files:
    file_path = os.path.join(trusted_path, parquet_file)
    table_name = os.path.splitext(parquet_file)[0]

    try:
        df = pd.read_parquet(file_path)

        if table_name in schemas:
            for col, dtype in schemas[table_name].items():
                if col in df.columns:
                    if dtype == Date:
                        df[col] = pd.to_datetime(df[col])
                    elif dtype == Integer:
                        df[col] = df[col].astype("int")
                    elif dtype == Float or dtype == DECIMAL:
                        df[col] = df[col].astype("float")
                    elif isinstance(dtype, String):
                        df[col] = df[col].astype(str)

        refined_file_path = os.path.join(refined_path, parquet_file)
        df.to_parquet(refined_file_path, index=False)
        print(f"‚úÖ Arquivo refinado salvo: {refined_file_path}")

        df.to_sql(table_name, engine, schema="meu_schema", if_exists="replace", index=False, dtype=schemas.get(table_name, {}))
        print(f"‚úÖ Dados do arquivo {parquet_file} carregados na tabela '{table_name}' com sucesso!")
    
    except Exception as e:
        print(f"‚ùå Erro ao processar o arquivo {parquet_file}: {e}")
```

### Conclus√£o

O script load.py automatiza a carga de dados transformados para o banco de dados PostgreSQL, garantindo:

- Verifica√ß√£o e cria√ß√£o do schema meu_schema.
- Processamento correto dos arquivos Parquet.
- Aplica√ß√£o dos tipos corretos aos dados.
- Persist√™ncia dos dados no banco de forma eficiente.
- Com isso, os dados ficam prontos para serem utilizados em an√°lises e dashboards de BI.


## Consultas Anal√≠ticas

### **1. Receita total por categoria de produto.**

```sql
SELECT
    p.categoria,
    SUM(t.quantidade * p.preco) AS receita_total
FROM meu_schema.transacoes t
JOIN meu_schema.produtos p ON t.id_produto = p.id_produto
GROUP BY p.categoria
ORDER BY receita_total DESC;
```
* üîé O que retorna?
  - üëâ Retorna a receita total gerada por cada categoria de produto.
  - üëâ Faz a soma (SUM) do total vendido (quantidade * preco) para cada categoria.
  - üëâ Ordena da maior para a menor receita (ORDER BY receita_total DESC).

[Receita total por categoria de produto](sql/Receita%20total%20por%20categoria%20de%20produto.csv)


![Receita total por categoria de produto](image/Receita%20total%20por%20categoria%20de%20produto.png)



#### Poss√≠veis otimiza√ß√µes:

* √çndices para acelerar os JOINs e filtros
  - Criar um √≠ndice para a chave de jun√ß√£o (id_produto) melhora a performance:

```sql
CREATE INDEX idx_transacoes_id_produto ON meu_schema.transacoes(id_produto);
CREATE INDEX idx_produtos_id_produto ON meu_schema.produtos(id_produto);
```

* Indexa√ß√£o na coluna categoria
  - Se o agrupamento por categoria for frequente, um √≠ndice pode ajudar:

```sql
CREATE INDEX idx_produtos_categoria ON meu_schema.produtos(categoria);
```

* Reduzir leituras desnecess√°rias
  - Em vez de calcular SUM(t.quantidade * p.preco), pode-se pr√©-calcular os valores unit√°rios na tabela transacoes, evitando multiplica√ß√£o repetida.



### **Top 5 produtos mais vendidos em um per√≠odo de tempo.**

```sql
SELECT
    p.id_produto,
    p.nome_produto,
    SUM(t.quantidade) AS total_vendido
FROM meu_schema.transacoes t
JOIN meu_schema.produtos p ON t.id_produto = p.id_produto
WHERE t.data_transacao BETWEEN '2023-11-01' AND '2024-02-01'
GROUP BY p.id_produto, p.nome_produto
ORDER BY total_vendido DESC
LIMIT 5;
```

* üîé O que retorna?
  - üëâ Lista os 5 produtos mais vendidos entre 01/11/2023 e 01/02/2024.
  - üëâ Soma (SUM) a quantidade vendida (total_vendido).
  - üëâ Ordena do mais vendido para o menos (ORDER BY total_vendido DESC).
  - üëâ Limita para os Top 5 (LIMIT 5).

[Top 5 produtos mais vendidos em um per√≠odo de tempo](sql/Top%205%20produtos%20mais%20vendidos%20em%20um%20per√≠odo%20de%20tempo.csv)

![Top 5 produtos mais vendidos em um per√≠odo de tempo](image/Top%205%20produtos%20mais%20vendidos%20em%20um%20per√≠odo%20de%20tempo..png)


#### Poss√≠veis otimiza√ß√µes:

* √çndices para acelerar os JOINs e filtros
  - √çndice na coluna de data (data_transacao), j√° que estamos filtrando um intervalo de tempo:

```sql
CREATE INDEX idx_transacoes_data ON meu_schema.transacoes(data_transacao);
```

* √çndice na chave de jun√ß√£o (id_produto)

```sql
CREATE INDEX idx_transacoes_id_produto ON meu_schema.transacoes(id_produto);
```

* Particionamento da tabela
  - Se transacoes tiver milh√µes de registros, pode-se particionar a tabela por m√™s ou ano:

```sql
CREATE TABLE meu_schema.transacoes_2023_11 PARTITION OF meu_schema.transacoes
FOR VALUES FROM ('2023-11-01') TO ('2023-11-30');
```

- Isso acelera buscas em per√≠odos espec√≠ficos.

* Usar LIMIT corretamente
  - O ORDER BY total_vendido DESC LIMIT 5 pode ser otimizado com um √≠ndice sobre quantidade para acelerar a ordena√ß√£o.


### **N√∫mero de clientes ativos (que fizeram pelo menos 1 compra) nos √∫ltimos 3 meses.**

1. Query 1 (Usa CURRENT_DATE): 

```sql
SELECT 
    COUNT(DISTINCT t.id_cliente) AS clientes_ativos
FROM meu_schema.transacoes t
WHERE t.data_transacao >= CURRENT_DATE - INTERVAL '3 months';
```

* CURRENT_DATE retorna apenas a data atual, sem considerar o hor√°rio (exemplo: 2025-02-27 00:00:00).
* Isso significa que a query sempre compara com o in√≠cio do dia.
* Se voc√™ rodar essa consulta √†s 23:59, ainda estar√° pegando todos os dados desde o in√≠cio do dia 3 meses atr√°s.

‚úÖ Mais confi√°vel se voc√™ quer considerar dias inteiros (ou seja, todos os registros de um determinado dia).

[N√∫mero de clientes ativos (que fizeram pelo menos 1 compra) nos √∫ltimos 3 meses(Current_date)](sql/N√∫mero%20de%20clientes%20ativos%20(que%20fizeram%20pelo%20menos%201%20compra)%20nos%20√∫ltimos%203%20meses_current_date.csv)

![N√∫mero de clientes ativos (que fizeram pelo menos 1 compra) nos √∫ltimos 3 meses(current_date)](image/N√∫mero%20de%20clientes%20ativos%20(que%20fizeram%20pelo%20menos%201%20compra)%20nos%20√∫ltimos%203%20meses.current_date..png)


1. Query 2 (Usa NOW()):

```sql
SELECT
    COUNT(DISTINCT t.id_cliente) AS clientes_ativos
FROM meu_schema.transacoes t
WHERE t.data_transacao >= NOW() - INTERVAL '3 months';
```

* NOW() retorna a data e hora exata do momento em que a query √© executada.
* Isso significa que a compara√ß√£o ser√° feita com a hora e minuto exato de 3 meses atr√°s.
* Se voc√™ rodar a consulta √†s 16:30 de hoje, pegar√° dados desde as 16:30 de 3 meses atr√°s.

‚úÖ Mais confi√°vel se voc√™ precisa de precis√£o hor√°ria (por exemplo, se as transa√ß√µes ocorrem ao longo do dia e cada hora importa).

[N√∫mero de clientes ativos (que fizeram pelo menos 1 compra) nos √∫ltimos 3 meses(now)](sql/N√∫mero%20de%20clientes%20ativos%20(que%20fizeram%20pelo%20menos%201%20compra)%20nos%20√∫ltimos%203%20meses_now.csv)

![N√∫mero de clientes ativos (que fizeram pelo menos 1 compra) nos √∫ltimos 3 meses(now)](image//N√∫mero%20de%20clientes%20ativos%20(que%20fizeram%20pelo%20menos%201%20compra)%20nos%20√∫ltimos%203%20meses.%20now..png)


#### Considera√ß√µes: 

* Ambas as queries contam clientes ativos nos √∫ltimos 3 meses, mas a diferen√ßa est√° na fun√ß√£o usada para a data de corte:
  - Se seu sistema trabalha com dados agregados por dia (exemplo: relat√≥rios di√°rios), use `CURRENT_DATE` para garantir que todas as transa√ß√µes do dia sejam contadas.
  - Se precisar de precis√£o at√© a hora e minuto exatos, use `NOW()`.

#### Poss√≠veis otimiza√ß√µes:

* √çndice na coluna data_transacao
  - Como estamos filtrando por data, um √≠ndice ajuda:

```sql
CREATE INDEX idx_transacoes_data_cliente ON meu_schema.transacoes(data_transacao, id_cliente);
```

* Criar uma tabela de clientes ativos
  - Se essa m√©trica for usada frequentemente, criar uma tabela materializada reduz o tempo de execu√ß√£o:

```sql
CREATE MATERIALIZED VIEW meu_schema.clientes_ativos AS
SELECT id_cliente, MAX(data_transacao) AS ultima_compra
FROM meu_schema.transacoes
GROUP BY id_cliente;
```
- Assim, ao inv√©s de rodar a query completa, basta consultar:

```sql
SELECT COUNT(*) FROM meu_schema.clientes_ativos WHERE ultima_compra >= CURRENT_DATE - INTERVAL '3 months';
```

* Usar EXPLAIN ANALYZE para testar otimiza√ß√µes
  - Sempre teste suas consultas para verificar se os √≠ndices est√£o sendo usados corretamente:

```sql
EXPLAIN ANALYZE SELECT COUNT(DISTINCT t.id_cliente) FROM meu_schema.transacoes t WHERE t.data_transacao >= CURRENT_DATE - INTERVAL '3 months';
```


## 1. Otimiza√ß√£o do Pipeline

### 1.1 Indexa√ß√£o nas Tabelas do Banco de Dados

* Para melhorar o desempenho das consultas, √© recomend√°vel Adicionar √≠ndices nas colunas frequentemente utilizadas para filtragem e jun√ß√£o:

```sql
CREATE INDEX idx_clientes_email ON meu_schema.clientes(email);
CREATE INDEX idx_transacoes_cliente ON meu_schema.transacoes(id_cliente);
CREATE INDEX idx_transacoes_produto ON meu_schema.transacoes(id_produto);
```

* Benef√≠cio:
  - Melhora a performance de consultas frequentes no banco.

### 1.2 Estrat√©gias de Particionamento


* Particionar a tabela de transa√ß√µes por data para facilitar consultas hist√≥ricas e melhorar performance:

```sql
CREATE TABLE meu_schema.transacoes (
    id_transacao INT PRIMARY KEY,
    id_cliente INT REFERENCES meu_schema.clientes(id_cliente),
    id_produto INT REFERENCES meu_schema.produtos(id_produto),
    quantidade INT,
    data_transacao DATE
) PARTITION BY RANGE (data_transacao);
```
* Benef√≠cio:
  - Reduz a carga computacional ao processar dados hist√≥ricos.


### 1.3 Paraleliza√ß√£o no Processo de Transforma√ß√£o

* O script transform.py processa os arquivos sequencialmente. Para aumentar a efici√™ncia, √© poss√≠vel utilizar paraleliza√ß√£o ao carregar e transformar os dados.
* Utilizando a biblioteca `multiprocessing` do Python para paralelizar a carga dos arquivos CSV:

```python
from multiprocessing import Pool

def processar_arquivo(arquivo):
    df = pd.read_csv(os.path.join("data/raw", arquivo))
    return df

arquivos = ["cliente.csv", "produtos.csv", "transacoes.csv"]
with Pool(processes=3) as pool:
    resultados = pool.map(processar_arquivo, arquivos)
```
* Benef√≠cio:
  - Reduz o tempo de processamento ao utilizar m√∫ltiplos n√∫cleos da CPU.


## Decis√µes T√©cnicas

1. **Uso do PostgreSQL:**
  - Escolhido por sua robustez, suporte a transa√ß√µes ACID e compatibilidade com ferramentas de an√°lise.

2. **Armazenamento em Parquet:**
  - O formato Parquet foi escolhido para a camada trusted/ devido √† sua efici√™ncia em armazenamento e velocidade de leitura.

3. **Uso do Docker & Docker Compose:**
  - Facilita a reprodutibilidade do ambiente e simplifica a implanta√ß√£o do ETL.

4. **Execu√ß√£o Sequencial dos Scripts ETL:**
   - Mant√©m a simplicidade do fluxo, mas pode ser otimizado utilizando paralelismo.
